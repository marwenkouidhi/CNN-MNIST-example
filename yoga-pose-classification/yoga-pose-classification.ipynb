{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d442d2fb-297f-4f50-93b9-4ad9f52f21d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from PIL import ImageFile, Image\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e4536a-9e40-4b77-b2b2-e6bb36a01047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_image_in_actual_size(img_path):\n",
    "    dpi = 100\n",
    "    img_data = plt.imread(img_path)\n",
    "    height, width, depth = img_data.shape\n",
    "    # What size does the figure need to be in inches to fit\n",
    "    # the image?\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "    # Create a figure of the right size with one axis that\n",
    "    # takes up the full figure\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "    # Hide spines, ticks, etc.\n",
    "    ax.axis('off')\n",
    "    # Display the image.\n",
    "    ax.imshow(img_data, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9a04ef-7875-49ec-9eca-4261aca2f4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'DATASET/TRAIN/downdog/'\n",
    "images = ['00000128.jpg', '00000130.jpg']\n",
    "\n",
    "# for img in images:\n",
    "#     display_image_in_actual_size(path + img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdf8cc-6bb6-46f0-bf26-fb7e1485f6dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5db8c01-d34a-40dc-a559-30adae4e8d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this cell we've defined:\n",
    "#  - the directory path of the dataset,\n",
    "#  - the batch size for loading images,\n",
    "#  - the image size (all images will be resized to this size) \n",
    "#  - the proportion of the dataset to be used for validation\n",
    "\n",
    "TRAIN_PATH = 'DATASET/TRAIN/'\n",
    "TEST_PATH = 'DATASET/TRAIN/'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "pixels =128\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "\n",
    "VALIDATTION_PERCENTAGE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68d4a13-79da-4ad3-b95d-63ab729e0b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cf69e89-210f-4bf0-aff6-d6c76689206d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 862 images belonging to 5 classes.\n",
      "Found 213 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE, interpolation=\"bilinear\", color_mode=\"grayscale\")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(TRAIN_PATH, subset=\"training\", shuffle=True,  **dataflow_kwargs)\n",
    "validation_generator = datagen.flow_from_directory(TRAIN_PATH, subset=\"validation\", shuffle=False,  **dataflow_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a6dd5e-67c2-4863-909e-f5491c5c861c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 256, 256, 1), 27)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = next(train_generator)\n",
    "image.shape, len(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e60d97a5-b910-438c-bfd5-4bc9f70ac8ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1075 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "testgen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
    "test_generator = datagen.flow_from_directory(TEST_PATH, shuffle=False,  **dataflow_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cffa9f94-18e6-4003-8ae6-13fcbccbff89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 256, 1)\n",
      "(32, 5)\n"
     ]
    }
   ],
   "source": [
    "# The output is represented as NumPy arrays. For a batch of\n",
    "# images, the sample size is 32, with 224 pixels in height and\n",
    "# width and three channels representing RGB color space.\n",
    "\n",
    "# For the label batch, there are likewise 32 samples. Each row\n",
    "# is onehot encoded to represent which of the five classes it belongs to.\n",
    "\n",
    "for image_batch, labels_batch in train_generator:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca9a9a14-7e6f-48ae-83ad-7f79b9608459",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'downdog', 1: 'goddess', 2: 'plank', 3: 'tree', 4: 'warrior2'}\n"
     ]
    }
   ],
   "source": [
    "# retrieve the lookup dictionary of labels\n",
    "labels_idx = (train_generator.class_indices)\n",
    "idx_labels = dict((v,k) for k,v in labels_idx.items())\n",
    "print(idx_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59233285-3d13-4802-89ec-a35da281fcec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the idx_labels dictionary\n",
    "with open('prediction_lookup.pickle', 'wb') as handle:\n",
    "    pickle.dump(idx_labels, handle,\n",
    "    protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a847be-01d6-400d-bca8-f38dbbb6c378",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'downdog', 1: 'goddess', 2: 'plank', 3: 'tree', 4: 'warrior2'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('prediction_lookup.pickle', 'rb') as handle:\n",
    "    lookup = pickle.load(handle)\n",
    "lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88824267-c3d7-498e-a6ee-1ad9d806cd4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f32abdcf-625a-406a-8d58-d4e82bbe5fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 23:24:44.381215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-18 23:24:44.381625: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# define input shape\n",
    "input_shape = (pixels, pixels, 1)\n",
    "\n",
    "# initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# add convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# flatten the output of the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# add fully connected layers with dropout\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57ae1802-a9a6-4b8a-8270-c49c98f2545a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d (Conv2D)                 (None, 254, 254, 32)          320         \n",
      "                                                                           \n",
      " max_pooling2d (MaxPooling2D)    (None, 127, 127, 32)          0           \n",
      "                                                                           \n",
      " conv2d_1 (Conv2D)               (None, 125, 125, 64)          18496       \n",
      "                                                                           \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 62, 62, 64)            0           \n",
      "                                                                           \n",
      " conv2d_2 (Conv2D)               (None, 60, 60, 128)           73856       \n",
      "                                                                           \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 30, 30, 128)           0           \n",
      "                                                                           \n",
      " flatten (Flatten)               (None, 115200)                0           \n",
      "                                                                           \n",
      " dense (Dense)                   (None, 16)                    1843216     \n",
      "                                                                           \n",
      " dropout (Dropout)               (None, 16)                    0           \n",
      "                                                                           \n",
      " dense_1 (Dense)                 (None, 16)                    272         \n",
      "                                                                           \n",
      " dropout_1 (Dropout)             (None, 16)                    0           \n",
      "                                                                           \n",
      " dense_2 (Dense)                 (None, 5)                     85          \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 1,936,245\n",
      "Trainable params: 1,936,245\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A brief summary of the model and parameters\n",
    "model.summary(line_length = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a41a4325-ce9d-47f8-bb51-c101a4cc5a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd1ec4c-e437-4505-93d0-0168ab6bed2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596f7c4-2bc4-4eef-a11c-dd46adb1afb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Early Stopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf49dac1-c511-4820-b066-4609c07a61db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',    \n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56a32d3-384a-4607-a77b-84dd7cf87720",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tensorboard Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c188b25-0d9f-42a1-a3bd-448ae020f93e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b15c2-4ccb-4b71-9c53-e64f7228a6e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trainning the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcab1a7c-468c-4886-8e9d-d28f10a1a0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 23:24:45.001294: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "/home/user01/miniconda3/envs/lab/lib/python3.11/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "2023-04-18 23:25:12.699143: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 - 30s - loss: 1.8240 - accuracy: 0.1972 - val_loss: 1.6071 - val_accuracy: 0.2347 - 30s/epoch - 1s/step\n",
      "Epoch 2/20\n",
      "27/27 - 30s - loss: 1.6087 - accuracy: 0.2135 - val_loss: 1.6041 - val_accuracy: 0.2864 - 30s/epoch - 1s/step\n",
      "Epoch 3/20\n",
      "27/27 - 29s - loss: 1.6054 - accuracy: 0.2378 - val_loss: 1.6013 - val_accuracy: 0.2441 - 29s/epoch - 1s/step\n",
      "Epoch 4/20\n",
      "27/27 - 29s - loss: 1.6149 - accuracy: 0.2413 - val_loss: 1.6018 - val_accuracy: 0.2441 - 29s/epoch - 1s/step\n",
      "Epoch 5/20\n",
      "27/27 - 29s - loss: 1.6015 - accuracy: 0.2483 - val_loss: 1.5939 - val_accuracy: 0.2441 - 29s/epoch - 1s/step\n",
      "Epoch 6/20\n",
      "27/27 - 29s - loss: 1.5923 - accuracy: 0.2541 - val_loss: 1.5862 - val_accuracy: 0.2441 - 29s/epoch - 1s/step\n",
      "Epoch 7/20\n",
      "27/27 - 28s - loss: 1.5745 - accuracy: 0.2773 - val_loss: 1.5621 - val_accuracy: 0.3474 - 28s/epoch - 1s/step\n",
      "Epoch 8/20\n",
      "27/27 - 28s - loss: 1.5359 - accuracy: 0.3492 - val_loss: 1.5154 - val_accuracy: 0.3427 - 28s/epoch - 1s/step\n",
      "Epoch 9/20\n",
      "27/27 - 28s - loss: 1.5212 - accuracy: 0.3202 - val_loss: 1.5018 - val_accuracy: 0.4038 - 28s/epoch - 1s/step\n",
      "Epoch 10/20\n",
      "27/27 - 28s - loss: 1.4731 - accuracy: 0.3573 - val_loss: 1.4450 - val_accuracy: 0.4460 - 28s/epoch - 1s/step\n",
      "Epoch 11/20\n",
      "27/27 - 28s - loss: 1.4760 - accuracy: 0.3654 - val_loss: 1.3800 - val_accuracy: 0.5023 - 28s/epoch - 1s/step\n",
      "Epoch 12/20\n",
      "27/27 - 30s - loss: 1.4403 - accuracy: 0.3794 - val_loss: 1.4023 - val_accuracy: 0.4977 - 30s/epoch - 1s/step\n",
      "Epoch 13/20\n",
      "27/27 - 29s - loss: 1.3835 - accuracy: 0.4408 - val_loss: 1.3320 - val_accuracy: 0.4977 - 29s/epoch - 1s/step\n",
      "Epoch 14/20\n",
      "27/27 - 28s - loss: 1.3141 - accuracy: 0.4629 - val_loss: 1.2921 - val_accuracy: 0.5634 - 28s/epoch - 1s/step\n",
      "Epoch 15/20\n",
      "27/27 - 28s - loss: 1.3249 - accuracy: 0.4559 - val_loss: 1.2014 - val_accuracy: 0.6103 - 28s/epoch - 1s/step\n",
      "Epoch 16/20\n",
      "27/27 - 28s - loss: 1.2637 - accuracy: 0.4849 - val_loss: 1.1787 - val_accuracy: 0.6009 - 28s/epoch - 1s/step\n",
      "Epoch 17/20\n",
      "27/27 - 28s - loss: 1.2099 - accuracy: 0.5116 - val_loss: 1.2232 - val_accuracy: 0.5869 - 28s/epoch - 1s/step\n",
      "Epoch 18/20\n",
      "27/27 - 28s - loss: 1.2388 - accuracy: 0.4942 - val_loss: 1.1483 - val_accuracy: 0.6244 - 28s/epoch - 1s/step\n",
      "Epoch 19/20\n",
      "27/27 - 28s - loss: 1.1866 - accuracy: 0.5070 - val_loss: 1.1677 - val_accuracy: 0.6103 - 28s/epoch - 1s/step\n",
      "Epoch 20/20\n",
      "27/27 - 29s - loss: 1.1270 - accuracy: 0.5615 - val_loss: 1.0976 - val_accuracy: 0.6620 - 29s/epoch - 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdff647bc10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "model.fit(\n",
    "    train_generator, \n",
    "    epochs = NUM_EPOCHS, \n",
    "    callbacks = [tensorboard_callback, early_stopping], \n",
    "    validation_data = validation_generator,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c2674-73e0-4108-ad5d-7ef0c0f84c60",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be975a65-ea09-4627-be5e-5a6577ef1e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 23:34:19.179862: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 11s 327ms/step - loss: 0.9306 - accuracy: 0.7144\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8121480b-cf80-4df9-9b38-c67ef8280741",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimages_plot\u001b[49m[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m\"\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print the correct label for the image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images_plot' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACuCAYAAABAzl3QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAB6ElEQVR4nO3SQQ0AIBDAMMC/58MDH7KkVbDH9szMgqDzOwBemZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS5Z5yTIvWeYly7xkmZcs85JlXrLMS9YFkywFWC2fpQ0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 502\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002362a9-f508-45fd-b6b6-6b663260c541",
   "metadata": {},
   "source": [
    "## Visualizing in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a5c4426-99cc-4fcc-b669-d31e32085e59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12902), started 23:57:00 ago. (Use '!kill 12902' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-941fc45f4e89f534\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-941fc45f4e89f534\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['TENSORBOARD_BINARY'] = '/home/user01/miniconda3/envs/lab/bin/tensorboard'\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'logs/fit/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4a1c1-105a-4954-a5cf-c4795f032b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
