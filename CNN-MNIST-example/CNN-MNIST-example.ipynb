{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "Rlp5wUW_FDmH",
    "outputId": "375cb352-57f9-4bc4-aa5b-8a9824e8cad2"
   },
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before continuing with our model and training, our first job is to preprocess the dataset\n",
    "# This is a very important step in all of machine learning\n",
    "\n",
    "# The MNIST dataset is, in general, highly processed already - after all its 28x28 grayscale images of clearly visible digits\n",
    "# Thus, our preprocessing will be limited to scaling the pixel values, shuffling the data and creating a validation set\n",
    "\n",
    "# NOTE: When finally deploying a model in practice, it might be a good idea to include the prerpocessing as initial layers\n",
    "# In that way, the users could just plug the data (images) directly, instead of being required to resize/rescale it before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining some constants/hyperparameters\n",
    "BUFFER_SIZE = 70_000 # for reshuffling\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Downloading the MNIST dataset\n",
    "\n",
    "# When 'with_info' is set to True, tfds.load() returns two variables: \n",
    "# - the dataset (including the train and test sets) \n",
    "# - meta info regarding the dataset itself\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Extracting the train and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Creating a function to scale our image data (it is recommended to scale the pixel values in the range [0,1] )\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining the size of the validation set\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining the size of the test set\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Reshuffling the dataset\n",
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into training + validation\n",
    "train_data = train_and_validation_data.skip(num_validation_samples)\n",
    "validation_data = train_and_validation_data.take(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Batching the data\n",
    "# NOTE: For proper functioning of the model, we need to create one big batch for the validation and test sets\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) \n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 23:41:41.704978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-04-16 23:41:41.705570: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the numpy arrays from the validation data for the calculation of the Confusion Matrix\n",
    "for images, labels in validation_data:\n",
    "    images_val = images.numpy()\n",
    "    labels_val = labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have preprocessed the dataset, we can define our CNN and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Outlining the model/architecture of our CNN\n",
    "# CONV -> MAXPOOL -> CONV -> MAXPOOL -> FLATTEN -> DENSE\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    # (2,2) is the default pool size so we could have just used MaxPooling2D() with no explicit arguments\n",
    "    tf.keras.layers.Conv2D(50, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10) # You can apply softmax activation here, see below for comentary\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d_4 (Conv2D)               (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 12, 12, 50)            0           \n",
      "                                                                           \n",
      " conv2d_5 (Conv2D)               (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 5, 5, 50)              0           \n",
      "                                                                           \n",
      " flatten_2 (Flatten)             (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense_2 (Dense)                 (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A brief summary of the model and parameters\n",
    "model.summary(line_length = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "\n",
    "# In general, our model needs to output probabilities of each class, \n",
    "# which can be achieved with a softmax activation in the last dense layer\n",
    "\n",
    "# However, when using the softmax activation, the loss can rarely be unstable\n",
    "\n",
    "# Thus, instead of incorporating the softmax into the model itself,\n",
    "# we use a loss calculation that automatically corrects for the missing softmax\n",
    "\n",
    "# That is the reason for 'from_logits=True'\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Compiling the model with Adam optimizer and the cathegorical crossentropy as a loss function\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    \n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    \n",
    "    # Closing the figure prevents it from being displayed directly inside the notebook.\n",
    "    plt.close(figure)\n",
    "    \n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    \n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a file writer variable for logging purposes\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + '/cm')\n",
    "\n",
    "def log_confusion_matrix(epoch, logs):\n",
    "    # Use the model to predict the values from the validation dataset.\n",
    "    test_pred_raw = model.predict(images_val)\n",
    "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "    # Calculate the confusion matrix.\n",
    "    cm = sklearn.metrics.confusion_matrix(labels_val, test_pred)\n",
    "    \n",
    "    # Log the confusion matrix as an image summary.\n",
    "    figure = plot_confusion_matrix(cm, class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the callbacks for logging the confusion matrix \n",
    "log_dir = \"logs/fit/\" + \"run-1\"\n",
    "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',    \n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 23:41:43.615555: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-04-16 23:41:43.616149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 14s - loss: 0.2711 - accuracy: 0.9218 - val_loss: 0.0796 - val_accuracy: 0.9762 - 14s/epoch - 33ms/step\n",
      "Epoch 2/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0668 - accuracy: 0.9801 - val_loss: 0.0434 - val_accuracy: 0.9857 - 15s/epoch - 35ms/step\n",
      "Epoch 3/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0512 - accuracy: 0.9846 - val_loss: 0.0441 - val_accuracy: 0.9872 - 15s/epoch - 36ms/step\n",
      "Epoch 4/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0421 - accuracy: 0.9866 - val_loss: 0.0296 - val_accuracy: 0.9922 - 15s/epoch - 36ms/step\n",
      "Epoch 5/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.0274 - val_accuracy: 0.9912 - 15s/epoch - 36ms/step\n",
      "Epoch 6/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0306 - accuracy: 0.9910 - val_loss: 0.0220 - val_accuracy: 0.9938 - 15s/epoch - 35ms/step\n",
      "Epoch 7/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0265 - accuracy: 0.9922 - val_loss: 0.0234 - val_accuracy: 0.9940 - 15s/epoch - 35ms/step\n",
      "Epoch 8/20\n",
      "188/188 [==============================] - 1s 3ms/step\n",
      "422/422 - 15s - loss: 0.0244 - accuracy: 0.9925 - val_loss: 0.0226 - val_accuracy: 0.9923 - 15s/epoch - 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6b0428c90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "model.fit(\n",
    "    train_data, \n",
    "    epochs = NUM_EPOCHS, \n",
    "    callbacks = [cm_callback, tensorboard_callback, early_stopping], \n",
    "    validation_data = validation_data,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFoXl2txFDmV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 23:43:42.641046: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-04-16 23:43:42.641464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 772ms/step - loss: 0.0326 - accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFoXl2txFDmV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0326. Test accuracy: 99.02%\n"
     ]
    }
   ],
   "source": [
    "# Printing the test results\n",
    "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting images and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 23:43:43.447706: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-16 23:43:43.448140: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "# Split the test_data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "# Reshape the images into 28x28 form, suitable for matplotlib (original dimensions: 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACuCAYAAABAzl3QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFG0lEQVR4nO3dzyt0bRzH8TP3UCg/mpUFVlaSkqSIFBaSnSj+AT9q8i8oVrKSZG+jYSNZKFESs5CywkoWFsiPUsIwz+Kpp56u7+jyY9w+Z96v5bfLmWvq3ek+58zcE0mn0+kAEPTnb28A+CzihSzihSzihSzihSzihSzihSzihSzihaw834WRSCSb+wD+4/vQlzMvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZHn/9jDCpbCw0Jx3dHR4H+Px8dGZbW5ufnpPH8WZF7KIF7KIF7KIF7Jy7oItGo06s8nJSXPtwMCA93GtC5VMxz0/Pzfn6XTa+/Ws92HNgiAIJiYmnNng4KC5tqKiwnsPqVTKme3t7Zlr+/v7ndnl5aX3a1k480IW8UIW8UIW8UIW8UJWJO15iRuJRLK9l29VXFxszhOJhDPr6urK9nb+Jx6Pm/Pj42Nn1tfXZ65tbGx0ZvX19V/b2A/LdHfE964LZ17IIl7IIl7IIl7ICsUFW2VlpTNbW1sz19bW1nof9+LiwpmNjIyYa5uampzZ8PCwuTYWi3nv4aclk0lnNjMzY669ubn50mttbW2Zcy7YEHrEC1nEC1nEC1nEC1mhuNuwsbHhzD7yLVjrrkIQBEFPT48zOzo68j7uzs6OOW9ubvY+xkc8PT2Z88XFRWc2NTVlrrU+IG59SzibuNuA0CNeyCJeyCJeyArFt4c7OzudWaZ/9L++vnr9fRAEwcnJydc29g1ub2/N+dLSkjObnp42156dnX3nln4NzryQRbyQRbyQRbyQRbyQFYrHw9ZbeHt7M9e+vLw4s4KCgm/fUxAEQV1dnTkfHx835/f3985sfn7eXHt6evrpff12PB5G6BEvZBEvZBEvZIXigm12dtaZjY6Oev/92NiYObc+B/vw8OC/MXwKF2wIPeKFLOKFLOKFLOKFrFDcbSgpKXFmy8vL5tqPfKt4e3vbmXV3d5trn5+fvY+L93G3AaFHvJBFvJBFvJAVigs2S1lZmTlfWVlxZu3t7d7H3d/fN+ctLS3ex8D7uGBD6BEvZBEvZBEvZBEvZIX2bkMmRUVFzmx9fd1c29ra6n3chYUFZ5bpW8I8Sn4fdxsQesQLWcQLWcQLWTl3wWYpLS0154lEwpl95PPAbW1t5nx3d9f7GLmICzaEHvFCFvFCFvFCFvFCFncb3tHQ0ODMrG8UB4H92Hlubs5cG4/Hv7SvsONuA0KPeCGLeCGLeCErFL89nC0HBwfO7O7uzlxrXbBVVVWZa6PRqDm3fhcZmXHmhSzihSzihSzihSzihaycu9uQl+e+5Uy/75tMJp1ZLBbzfq3e3l5znukYV1dX3scGZ14II17IIl7IIl7IyrnP81oXbIeHh+bampqarOyhvLzcnHPB9i8+z4vQI17IIl7IIl7IIl7IyrnHw6lUypll+v/HhoaGnFl+fr65trq62pmtrq6aa6+vr9/bIjxx5oUs4oUs4oUs4oWsnHs8jN+Px8MIPeKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLO9vD/t+QBj4KZx5IYt4IYt4IYt4IYt4IYt4IYt4IYt4IYt4Iesf6XIjavaLCioAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 502\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 31912), started 0:07:49 ago. (Use '!kill 31912' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3adac265b2925ec4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3adac265b2925ec4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['TENSORBOARD_BINARY'] = '/home/user01/miniconda3/envs/lab/bin/tensorboard'\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'logs/fit/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "defaultNotebook.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
