{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "Rlp5wUW_FDmH",
    "outputId": "375cb352-57f9-4bc4-aa5b-8a9824e8cad2"
   },
   "outputs": [],
   "source": [
    "# Importing the relevant packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before continuing with our model and training, our first job is to preprocess the dataset\n",
    "# This is a very important step in all of machine learning\n",
    "\n",
    "# The MNIST dataset is, in general, highly processed already - after all its 28x28 grayscale images of clearly visible digits\n",
    "# Thus, our preprocessing will be limited to scaling the pixel values, shuffling the data and creating a validation set\n",
    "\n",
    "# NOTE: When finally deploying a model in practice, it might be a good idea to include the prerpocessing as initial layers\n",
    "# In that way, the users could just plug the data (images) directly, instead of being required to resize/rescale it before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining some constants/hyperparameters\n",
    "BUFFER_SIZE = 70_000 # for reshuffling\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Downloading the MNIST dataset\n",
    "\n",
    "# When 'with_info' is set to True, tfds.load() returns two variables: \n",
    "# - the dataset (including the train and test sets) \n",
    "# - meta info regarding the dataset itself\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Extracting the train and test datasets\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Creating a function to scale our image data (it is recommended to scale the pixel values in the range [0,1] )\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining the size of the validation set\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining the size of the test set\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Reshuffling the dataset\n",
    "train_and_validation_data = train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into training + validation\n",
    "train_data = train_and_validation_data.skip(num_validation_samples)\n",
    "validation_data = train_and_validation_data.take(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Batching the data\n",
    "# NOTE: For proper functioning of the model, we need to create one big batch for the validation and test sets\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) \n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have preprocessed the dataset, we can define our CNN and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Outlining the model/architecture of our CNN\n",
    "# CONV -> MAXPOOL -> CONV -> MAXPOOL -> FLATTEN -> DENSE\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(50, 5, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    # (2,2) is the default pool size so we could have just used MaxPooling2D() with no explicit arguments\n",
    "    tf.keras.layers.Conv2D(50, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10) # You can apply softmax activation here, see below for comentary\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "___________________________________________________________________________\n",
      " Layer (type)                    Output Shape                  Param #     \n",
      "===========================================================================\n",
      " conv2d_2 (Conv2D)               (None, 24, 24, 50)            1300        \n",
      "                                                                           \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 50)            0           \n",
      "                                                                           \n",
      " conv2d_3 (Conv2D)               (None, 10, 10, 50)            22550       \n",
      "                                                                           \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 50)              0           \n",
      "                                                                           \n",
      " flatten_1 (Flatten)             (None, 1250)                  0           \n",
      "                                                                           \n",
      " dense_1 (Dense)                 (None, 10)                    12510       \n",
      "                                                                           \n",
      "===========================================================================\n",
      "Total params: 36,360\n",
      "Trainable params: 36,360\n",
      "Non-trainable params: 0\n",
      "___________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A brief summary of the model and parameters\n",
    "model.summary(line_length = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the loss function\n",
    "\n",
    "# In general, our model needs to output probabilities of each class, \n",
    "# which can be achieved with a softmax activation in the last dense layer\n",
    "\n",
    "# However, when using the softmax activation, the loss can rarely be unstable\n",
    "\n",
    "# Thus, instead of incorporating the softmax into the model itself,\n",
    "# we use a loss calculation that automatically corrects for the missing softmax\n",
    "\n",
    "# That is the reason for 'from_logits=True'\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Compiling the model with Adam optimizer and the cathegorical crossentropy as a loss function\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',    \n",
    "    min_delta = 0,\n",
    "    patience = 2,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "9S6uGLzkFDmP",
    "outputId": "6a5bad6b-035f-4f2e-a81c-8015e17001f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 - 14s - loss: 0.0086 - accuracy: 0.9972 - val_loss: 0.0053 - val_accuracy: 0.9987 - 14s/epoch - 32ms/step\n",
      "Epoch 2/20\n",
      "422/422 - 12s - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.0070 - val_accuracy: 0.9977 - 12s/epoch - 29ms/step\n",
      "Epoch 3/20\n",
      "422/422 - 12s - loss: 0.0083 - accuracy: 0.9971 - val_loss: 0.0030 - val_accuracy: 0.9993 - 12s/epoch - 28ms/step\n",
      "Epoch 4/20\n",
      "422/422 - 12s - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0072 - val_accuracy: 0.9978 - 12s/epoch - 28ms/step\n",
      "Epoch 5/20\n",
      "422/422 - 12s - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0055 - val_accuracy: 0.9983 - 12s/epoch - 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1457f8ebd0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the network\n",
    "model.fit(\n",
    "    train_data, \n",
    "    epochs = NUM_EPOCHS, \n",
    "    callbacks = [early_stopping, tensorboard_callback], \n",
    "    validation_data = validation_data,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFoXl2txFDmV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 17:08:10.760311: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-16 17:08:10.760799: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 697ms/step - loss: 0.0336 - accuracy: 0.9903\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFoXl2txFDmV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0336. Test accuracy: 99.03%\n"
     ]
    }
   ],
   "source": [
    "# Printing the test results\n",
    "print('Test loss: {0:.4f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting images and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 17:08:11.493721: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-16 17:08:11.494142: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n"
     ]
    }
   ],
   "source": [
    "# Split the test_data into 2 arrays, containing the images and the corresponding labels\n",
    "for images, labels in test_data.take(1):\n",
    "    images_test = images.numpy()\n",
    "    labels_test = labels.numpy()\n",
    "\n",
    "# Reshape the images into 28x28 form, suitable for matplotlib (original dimensions: 28x28x1)\n",
    "images_plot = np.reshape(images_test, (10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACuCAYAAABAzl3QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFG0lEQVR4nO3dzyt0bRzH8TP3UCg/mpUFVlaSkqSIFBaSnSj+AT9q8i8oVrKSZG+jYSNZKFESs5CywkoWFsiPUsIwz+Kpp56u7+jyY9w+Z96v5bfLmWvq3ek+58zcE0mn0+kAEPTnb28A+CzihSzihSzihSzihSzihSzihSzihSzihaw834WRSCSb+wD+4/vQlzMvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZBEvZHn/9jDCpbCw0Jx3dHR4H+Px8dGZbW5ufnpPH8WZF7KIF7KIF7KIF7Jy7oItGo06s8nJSXPtwMCA93GtC5VMxz0/Pzfn6XTa+/Ws92HNgiAIJiYmnNng4KC5tqKiwnsPqVTKme3t7Zlr+/v7ndnl5aX3a1k480IW8UIW8UIW8UIW8UJWJO15iRuJRLK9l29VXFxszhOJhDPr6urK9nb+Jx6Pm/Pj42Nn1tfXZ65tbGx0ZvX19V/b2A/LdHfE964LZ17IIl7IIl7IIl7ICsUFW2VlpTNbW1sz19bW1nof9+LiwpmNjIyYa5uampzZ8PCwuTYWi3nv4aclk0lnNjMzY669ubn50mttbW2Zcy7YEHrEC1nEC1nEC1nEC1mhuNuwsbHhzD7yLVjrrkIQBEFPT48zOzo68j7uzs6OOW9ubvY+xkc8PT2Z88XFRWc2NTVlrrU+IG59SzibuNuA0CNeyCJeyCJeyArFt4c7OzudWaZ/9L++vnr9fRAEwcnJydc29g1ub2/N+dLSkjObnp42156dnX3nln4NzryQRbyQRbyQRbyQRbyQFYrHw9ZbeHt7M9e+vLw4s4KCgm/fUxAEQV1dnTkfHx835/f3985sfn7eXHt6evrpff12PB5G6BEvZBEvZBEvZIXigm12dtaZjY6Oev/92NiYObc+B/vw8OC/MXwKF2wIPeKFLOKFLOKFLOKFrFDcbSgpKXFmy8vL5tqPfKt4e3vbmXV3d5trn5+fvY+L93G3AaFHvJBFvJBFvJAVigs2S1lZmTlfWVlxZu3t7d7H3d/fN+ctLS3ex8D7uGBD6BEvZBEvZBEvZBEvZIX2bkMmRUVFzmx9fd1c29ra6n3chYUFZ5bpW8I8Sn4fdxsQesQLWcQLWcQLWTl3wWYpLS0154lEwpl95PPAbW1t5nx3d9f7GLmICzaEHvFCFvFCFvFCFvFCFncb3tHQ0ODMrG8UB4H92Hlubs5cG4/Hv7SvsONuA0KPeCGLeCGLeCErFL89nC0HBwfO7O7uzlxrXbBVVVWZa6PRqDm3fhcZmXHmhSzihSzihSzihSzihaycu9uQl+e+5Uy/75tMJp1ZLBbzfq3e3l5znukYV1dX3scGZ14II17IIl7IIl7IyrnP81oXbIeHh+bampqarOyhvLzcnHPB9i8+z4vQI17IIl7IIl7IIl7IyrnHw6lUypll+v/HhoaGnFl+fr65trq62pmtrq6aa6+vr9/bIjxx5oUs4oUs4oUs4oWsnHs8jN+Px8MIPeKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLOKFLO9vD/t+QBj4KZx5IYt4IYt4IYt4IYt4IYt4IYt4IYt4IYt4Iesf6XIjavaLCioAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# The image to be displayed and tested\n",
    "i = 502\n",
    "\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.axis('off')\n",
    "plt.imshow(images_plot[i-1], cmap=\"gray\", aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Print the correct label for the image\n",
    "print(\"Label: {}\".format(labels_test[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obtain the model's predictions (logits)\n",
    "predictions = model.predict(images_test[i-1:i])\n",
    "\n",
    "# Convert those predictions into probabilities (recall that we incorporated the softmaxt activation into the loss function)\n",
    "probabilities = tf.nn.softmax(predictions).numpy()\n",
    "# Convert the probabilities into percentages\n",
    "probabilities = probabilities*100\n",
    "\n",
    "\n",
    "# Create a bar chart to plot the probabilities for each class\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=probabilities[0], tick_label=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17105), started 0:18:52 ago. (Use '!kill 17105' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6c68a3718e0c7407\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6c68a3718e0c7407\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['TENSORBOARD_BINARY'] = '/home/user01/miniconda3/envs/lab/bin/tensorboard'\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'logs/fit/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "defaultNotebook.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
